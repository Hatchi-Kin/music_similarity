{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "import json\n",
    "import music_tag\n",
    "import numpy as np\n",
    "\n",
    "from music_utils import *\n",
    "\n",
    "from pymilvus import connections, utility\n",
    "from pymilvus import Collection, DataType, FieldSchema, CollectionSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid files: 174 | Number of invalid files: 0\n"
     ]
    }
   ],
   "source": [
    "DATASET = Path(\"Music_Dataset\")\n",
    "# list the path of all the .pkl files\n",
    "pkl_files = list(DATASET.rglob('*.pkl'))\n",
    "# check if the .pkl files are valid\n",
    "valid_files = [check_file_info(pkl_file) for pkl_file in pkl_files]\n",
    "print(f\"Number of valid files: {sum(valid_files)} | Number of invalid files: {len(valid_files) - sum(valid_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique types 87: {<class 'numpy.ndarray'>}\n",
      "Unique types 512: {<class 'numpy.ndarray'>}\n"
     ]
    }
   ],
   "source": [
    "types87 = []\n",
    "types512 = []\n",
    "\n",
    "for pkl_path in pkl_files:\n",
    "    with pkl_path.open('rb') as f:\n",
    "        try:\n",
    "            content = pickle.load(f)\n",
    "            embed = content.get('embedding_512')\n",
    "            if embed is not None:\n",
    "                types87.append(type(embed))\n",
    "                types512.append(type(embed))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {pkl_path}: {e}\")\n",
    "\n",
    "unique_types87 = set(types87)\n",
    "unique_types512 = set(types512)\n",
    "\n",
    "print(f\"Unique types 87: {unique_types87}\")\n",
    "print(f\"Unique types 512: {unique_types512}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: 01_Rehab.mp3\n",
      "filepath: Music_Dataset/Amy_Whinehouse/01_Rehab.mp3\n",
      "folder: Music_Dataset/Amy_Whinehouse\n",
      "filesize: 8.28\n",
      "title: Rehab\n",
      "artist: Amy Winehouse\n",
      "album: Back To Black (Deluxe Edition)\n",
      "year: None\n",
      "tracknumber: 1\n",
      "genre: Pop\n",
      "predictions_87: [0.0137573  0.00787907 0.00550216 0.01150016 0.00887729 0.125214\n",
      " 0.00893105 0.04464517 0.00872509 0.03716161 0.0035134  0.00543459\n",
      " 0.01179131 0.00099303 0.00342366 0.02123244 0.00120388 0.00799035\n",
      " 0.00206727 0.00677059 0.00117313 0.00716032 0.02799957 0.00169101\n",
      " 0.00217736 0.00263762 0.01154235 0.01169348 0.01869302 0.01457836\n",
      " 0.00332781 0.02558908 0.0033474  0.25421107 0.00414976 0.04483763\n",
      " 0.00655554 0.00181256 0.04970998 0.02087118 0.06964471 0.01897717\n",
      " 0.02035891 0.00426782 0.00467592 0.00557946 0.07791467 0.01519128\n",
      " 0.00302347 0.00396947 0.1014649  0.0075494  0.02356417 0.01140175\n",
      " 0.03421957 0.00418526 0.01481703 0.03088906 0.00073046 0.00478129\n",
      " 0.00256177 0.00542741 0.00592084 0.00773648 0.34483925 0.0139124\n",
      " 0.08931171 0.00781765 0.0098254  0.0131669  0.01436414 0.0338596\n",
      " 0.07976741 0.03799864 0.29496342 0.0215735  0.02417226 0.06334528\n",
      " 0.04146439 0.01033164 0.00217549 0.00890091 0.0152643  0.00495013\n",
      " 0.02702439 0.03622036 0.00883478]\n",
      "embedding_512: [ 3.9481826   0.4036317   1.8639355   3.2139907   1.6428864   1.8117875\n",
      "  1.2732842   1.9337156   3.4868052   2.975228    1.9287527   3.7504914\n",
      "  2.8356102   1.810391    2.3101237   5.179215    1.7424775   2.304761\n",
      "  2.819865    1.8229485   2.8443785   1.9289002   1.7496979   3.6678612\n",
      "  2.2583861   2.4621599   1.9083177   2.2175062   1.1585002   2.6731603\n",
      "  2.1479533   2.8717625   2.1895182   2.3105803   1.6689758   3.0461166\n",
      "  0.817087    1.7564818   2.3050935   2.664179    3.1434476   3.7189326\n",
      "  2.8126123   2.1749055   0.86815757  3.3609076   3.7762842   1.6407863\n",
      "  2.1852305   3.0713518   1.3863271   2.0950298   2.203359   -0.5974827\n",
      "  1.7117021   1.8581578   2.9166393   1.9053845   3.0444381   1.4169586\n",
      "  2.2160141   3.4303606   1.7110419   2.3465629   1.8553492   2.7703648\n",
      "  2.9364293   2.7118797   2.1581523   3.0928078   3.2038472   1.4673116\n",
      "  1.7108728   2.0285084   3.502267    2.2879624   2.2365375   1.5582674\n",
      "  2.4717677   1.6395859   1.5818776   2.3203874   1.7552297   1.6090528\n",
      "  2.1656055   3.5709116   3.4365911   4.920493    1.9228717   3.69634\n",
      "  3.0939348   3.383481    1.6665716   2.5382855   2.1912687   1.7980051\n",
      "  4.3395314   2.1849704   1.9634976   3.5485103   4.2648134   1.9067026\n",
      "  1.4025838   1.2632198   3.1424015   2.3282      2.4864807   4.709903\n",
      "  1.7814767   1.5148026   1.3948218   1.8119547   3.2150738   1.9833183\n",
      "  3.007437    1.7639141   0.9562929   1.616917    1.2221831   1.549097\n",
      "  1.9938769   2.2398932   2.657858    1.9700812   3.2824922   2.1942797\n",
      "  1.5623825   0.9989589   4.9429727   1.9509727   2.0817962   1.5183965\n",
      "  2.2231603   2.7530372   2.1453764   1.6888064   2.5893757   2.2554564\n",
      "  2.2554002   3.1528144   2.7892253   1.8808897   2.1318645   2.8947153\n",
      "  3.1122358   3.8291805   2.2993283   1.0291078   3.4598753   1.7765357\n",
      "  2.1878154   3.6937833   1.7718048   2.6561077   2.3046608   1.6782005\n",
      "  1.1695775   3.2153792   2.2953327   2.6655664   2.9008815   1.0847889\n",
      "  0.9946574   2.1823475   2.6817253   1.9487203   0.9083599   2.5022132\n",
      "  3.382585    1.9161983   1.4620553   1.7651005   2.9491096   2.0393093\n",
      "  2.814543    3.0683064   2.6261787   1.2474451   2.2464578   1.6209714\n",
      "  2.4785223   3.464298    4.7788663   1.2664372   0.6887596   2.4116068\n",
      "  2.9753838   5.5399485   3.394912    2.1238453   2.8152432   2.9197547\n",
      "  2.6231306   1.1872615   2.7912824   3.1004305   1.6920134   2.3899515\n",
      "  2.8073275   2.1469777   2.0116525   4.9718146   2.538026    2.8314564\n",
      "  2.0980878   3.152044    3.1867247   2.6879065   2.64527     1.7399026\n",
      "  1.0754585   1.8218924   1.540457    2.9607582   4.006988    3.071221\n",
      "  1.9221679   3.2017527   3.191955    2.5588815   1.3246688   1.809759\n",
      "  0.9647941   1.8019968   3.8477247   2.2340994   1.3396721   2.8825574\n",
      "  3.1118765   1.7255323   3.8913014   2.3368688   3.934919    1.8702536\n",
      "  1.920251    1.5378152   1.6390945   2.4808216   1.6667023   3.4557662\n",
      "  2.9192736   3.239868    3.3511138   2.2687082   1.5132647   2.7169874\n",
      "  2.0854347   2.318573    2.9152024   2.1592126   2.770044    1.8478718\n",
      "  2.0126398   3.1042156   2.2654424   3.1095688   1.5746939   2.176922\n",
      "  1.8967618   2.113552    1.4029576   1.681367    2.1452532   3.9689658\n",
      "  2.50625     1.4478902   4.1439953   3.1220317   1.6392148   1.8602458\n",
      "  5.00182     2.799064    4.9631295   4.0935216   1.8674849   2.4837637\n",
      "  2.1436481   1.8848832   3.2714586   2.1099873   1.6771048   2.245465\n",
      "  3.270159    3.0454679   2.8766546   3.1505227   1.4992255   2.469449\n",
      "  2.0413752   1.6761091   2.0105832   0.65371555  2.359268    1.2384546\n",
      "  2.1104035   2.0265706   2.4355578   5.5783825   2.1750634   2.2729497\n",
      "  1.5968815   1.621453    1.8971215   2.9342074   3.0895324   3.3402014\n",
      "  2.7141922   3.055961    1.9452296   2.0943854   3.478657    2.3328238\n",
      "  1.5248882   2.288178    2.5266237   2.5670958   2.760533    2.0728009\n",
      "  3.5143318   2.8173954   3.778648    1.8792237   2.8674262   2.8906813\n",
      "  1.7561517   1.0549277   1.6831036   1.9117041   2.624103    2.800504\n",
      "  3.0396328   2.0491319   2.6577375   2.6068258   2.954892    1.8129942\n",
      "  2.823905    2.1280298   1.3633806   4.5739017   2.4727015   1.8313296\n",
      "  2.3541746   2.618178    1.909589    3.3981786   1.5248895   2.7199867\n",
      "  4.180656    2.3398793   4.242456    1.9154785   3.747888    1.9023687\n",
      "  3.7102063   1.0299721   2.0913534   3.3229043   2.250938    1.551622\n",
      "  1.1985308   1.3746096   2.5740762   2.685028    3.0451415   3.1467178\n",
      "  1.7613924   2.8283777   1.7858237   1.0524919   1.4901862   3.0108478\n",
      "  3.1121895   2.46996     2.4633708   4.432119    2.2552974   1.9604754\n",
      "  1.2859548   1.4486614   2.030907    3.0719044   2.4377759   1.298267\n",
      "  1.826173    1.6884733   2.7900186   2.7397158   2.4793277   2.4984787\n",
      "  3.6988413   2.427494    3.1738496   3.3819394   2.5227628   2.287957\n",
      "  1.70288     2.621042    1.874787    1.5463252   1.5382396   2.5837286\n",
      "  2.4978478   2.744658    4.330587    2.408425    1.1974003   3.940358\n",
      "  3.3021848   3.9652338   2.6822877   1.0042115   2.0704105   2.5605044\n",
      "  1.9151798   2.0743103   1.7647102   1.4376976   1.4456198   2.0824244\n",
      "  2.867293    1.352356    2.0015497   1.2616128   2.1920762   2.1463768\n",
      "  2.0060065   2.138009    1.7911216   2.010432    2.6773856   1.6746535\n",
      "  3.00342     2.6409757   3.1089995   3.1120071   2.1469424   1.4937055\n",
      "  1.5062466   2.0831237   3.4442904   3.915113    1.5030396   3.2180862\n",
      "  2.5750456   1.1720978   1.2556975   2.23845     3.9192863   1.7145652\n",
      "  1.4837234   1.7581472   2.0253155   3.151908    2.1957986   2.5768428\n",
      "  1.6048177   2.4348018   1.2835032   2.85747     3.4249117   3.735767\n",
      "  2.8765545   2.3234673   2.6304784   2.4140484   2.9967232   2.7611704\n",
      "  0.93318135  2.1518922   2.0645316   4.0189586   2.4488015   1.680321\n",
      "  2.0743892   1.697581    1.5979751   3.4841292   2.5283647   2.0573566\n",
      "  1.8266487   2.5469363   1.5213765   2.9467483   3.3598096   2.6818488\n",
      "  2.7391682   2.3408177   2.856536    3.081986    2.2250602   2.948457\n",
      "  0.9900011   1.7971299   1.7934834   3.5051827   3.3689265   2.8396094\n",
      "  2.6278672   2.2410696   2.0867805   4.460662    3.0038567   2.344035\n",
      "  2.8997166   2.359103    2.2338839   3.3988256   2.0523887   1.6837884\n",
      "  1.6077118   4.0023828 ]\n",
      "top_5_genres: ['pop', 'rock', 'electronic', 'alternative', 'indie']\n"
     ]
    }
   ],
   "source": [
    "print_info(pick_random_mp3(DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Milvus Client ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = \"https://in03-efa63c0579a14a1.api.gcp-us-west1.zillizcloud.com\"\n",
    "\n",
    "TOKEN = \"e58021f476f7b39e5d84eb5c804e27bfec1a7fb89b6e01f7560ac57877be699b9b1f109a2ba8fabefd2fa26f2efab109ebdd79f0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to DB: https://in03-efa63c0579a14a1.api.gcp-us-west1.zillizcloud.com\n"
     ]
    }
   ],
   "source": [
    "# connect to milvus\n",
    "connections.connect(\"default\",\n",
    "                    uri=URI,\n",
    "                    token=TOKEN)\n",
    "print(f\"Connecting to DB: {URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## predictions_87 ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['predictions_87', 'embeddings_512']\n"
     ]
    }
   ],
   "source": [
    "print(utility.list_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embeddings_512']\n"
     ]
    }
   ],
   "source": [
    "# Check if the collection exists\n",
    "check_collection_name = 'predictions_87'\n",
    "\n",
    "check_collection = utility.has_collection(check_collection_name )\n",
    "if check_collection:\n",
    "    drop_result = utility.drop_collection(check_collection_name )\n",
    "\n",
    "print(utility.list_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating example collection: predictions_87\n",
      "Schema: {'auto_id': False, 'description': '', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': False}, {'name': 'path', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 280}}, {'name': 'title', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 220}}, {'name': 'artist', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 120}}, {'name': 'album', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 240}}, {'name': 'predictions', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 87}}, {'name': 'top_5_genres', 'description': '', 'type': <DataType.ARRAY: 22>, 'params': {'max_length': 100, 'max_capacity': 5}, 'element_type': <DataType.VARCHAR: 21>}], 'enable_dynamic_field': True}\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# create a collection for prediction_87\n",
    "collection_name = \"predictions_87\"\n",
    "dimension = 87\n",
    "\n",
    "# Define the schema\n",
    "schema = CollectionSchema(\n",
    "    fields=[\n",
    "        FieldSchema(name=\"id\",dtype=DataType.INT64,is_primary=True,auto_id=False,max_length=100),\n",
    "        FieldSchema(name=\"path\", dtype=DataType.VARCHAR, max_length=280),\n",
    "        FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=220),\n",
    "        FieldSchema(name=\"artist\", dtype=DataType.VARCHAR, max_length=120),\n",
    "        FieldSchema(name=\"album\", dtype=DataType.VARCHAR, max_length=240),\n",
    "        FieldSchema(name=\"predictions\", dtype=DataType.FLOAT_VECTOR, dim=dimension),\n",
    "        FieldSchema(name=\"top_5_genres\", dtype=DataType.ARRAY, element_type=DataType.VARCHAR, max_capacity=5, max_length=100),\n",
    "        ],\n",
    "    enable_dynamic_field=True,  # Optional, defaults to 'False'.\n",
    ")\n",
    "\n",
    "print(f\"Creating example collection: {collection_name}\")\n",
    "collection = Collection(collection_name, schema)\n",
    "print(f\"Schema: {schema}\")\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='app.log', filemode='w', format='%(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def process_file(args):\n",
    "    i, path = args\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            filepath = data.get(\"filepath\")\n",
    "            title = data.get(\"title\")\n",
    "            album = data.get(\"album\")\n",
    "            artist = data.get(\"artist\")\n",
    "            predictions = data.get(\"predictions_87\")\n",
    "            top_5_genres = data.get(\"top_5_genres\")\n",
    "\n",
    "        if isinstance(predictions, np.ndarray):\n",
    "            return (i, filepath, title, album, artist, predictions, top_5_genres)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 18 records into collection predictions_87.\n",
      "Inserted 36 records into collection predictions_87.\n",
      "Inserted 54 records into collection predictions_87.\n",
      "Inserted 72 records into collection predictions_87.\n",
      "Inserted 90 records into collection predictions_87.\n",
      "Inserted 108 records into collection predictions_87.\n",
      "Inserted 126 records into collection predictions_87.\n",
      "Inserted 144 records into collection predictions_87.\n",
      "Inserted 162 records into collection predictions_87.\n",
      "Inserted 180 records into collection predictions_87.\n"
     ]
    }
   ],
   "source": [
    "# Use a multiprocessing pool to process the files in parallel\n",
    "with Pool() as p:\n",
    "    results = p.map(process_file, enumerate(pkl_files))\n",
    "\n",
    "# Filter out None results\n",
    "results = [r for r in results if r is not None]\n",
    "\n",
    "# Batch size\n",
    "batch_size = 18\n",
    "\n",
    "fails = []\n",
    "\n",
    "# Insert the data into the collection\n",
    "for i in range(0, len(results), batch_size):\n",
    "    id_batch, path_batch, title_batch, album_batch, artist_batch, predictions_batch, top_5_genres_batch = zip(*results[i : i + batch_size])\n",
    "    documents = [\n",
    "        {\n",
    "            \"id\": id,\n",
    "            \"path\": path,\n",
    "            \"title\": title,\n",
    "            \"album\": album,\n",
    "            \"artist\": artist,\n",
    "            \"predictions\": predictions.tolist(),\n",
    "            \"top_5_genres\": top_5_genres\n",
    "        }\n",
    "        for id, path, title, album, artist, predictions, top_5_genres in zip(\n",
    "            id_batch, path_batch, title_batch, album_batch, artist_batch, predictions_batch, top_5_genres_batch,\n",
    "        )\n",
    "    ]\n",
    "    try:\n",
    "        collection.insert(documents)\n",
    "        print(f\"Inserted {i + batch_size} records into collection {collection.name}.\")\n",
    "    except Exception as e:\n",
    "        fails.append((i + batch_size, documents))\n",
    "        print(f\"Error inserting batch {i + batch_size} into collection {collection.name}. Error: {str(e)}\")\n",
    "\n",
    "# 2m20s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed batches: []\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed batches: {fails}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(code=0, message=)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = {\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\"nlist\": 128},\n",
    "}\n",
    "collection.create_index(field_name=\"predictions\", index_params=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random mp3: Music_Dataset/Java/01-Sex_Accordeon_Et_Alcool.pkl\n",
      "['hiphop', 'rap', 'electronic', 'experimental', 'reggae']\n",
      "id: 72, distance: 0.004329318180680275, entity: {'path': 'Music_Dataset/Soul_Square/02_-_Living_the_Dream_(feat._Justis).mp3'}\n",
      "id: 27, distance: 0.0067001529969275, entity: {'path': 'Music_Dataset/CYNE/02_The_River.mp3'}\n",
      "id: 133, distance: 0.007741497829556465, entity: {'path': 'Music_Dataset/Jurassic_5/005_Quality_Control.mp3'}\n",
      "id: 48, distance: 0.010727956891059875, entity: {'path': 'Music_Dataset/Dr_Dre/04. Still D.R.E.mp3'}\n",
      "id: 62, distance: 0.011915805749595165, entity: {'path': 'Music_Dataset/The_Streets/The Streets - 11 - The Irony Of It All.mp3'}\n",
      "id: 49, distance: 0.013396674767136574, entity: {'path': 'Music_Dataset/Dr_Dre/02. The Watcher.mp3'}\n",
      "id: 28, distance: 0.014681385830044746, entity: {'path': 'Music_Dataset/CYNE/11_Stomping_Ground.mp3'}\n",
      "id: 74, distance: 0.023833826184272766, entity: {'path': 'Music_Dataset/Soul_Square/03_-_Take_It_Back_(feat._Blezz).mp3'}\n",
      "id: 134, distance: 0.028507251292467117, entity: {'path': 'Music_Dataset/Jurassic_5/003_Great_Expectations.mp3'}\n",
      "id: 160, distance: 0.03411570191383362, entity: {'path': 'Music_Dataset/A_Tribe_Called_Quest/05_I_Left_My_Wallet_In_El_Segundo.mp3'}\n"
     ]
    }
   ],
   "source": [
    "random_song = pick_random_mp3(DATASET).with_suffix(\".pkl\")\n",
    "print(f\"Random mp3: {random_song}\")\n",
    "print(get_top_5_genres(random_song, \"mtg_jamendo_genre.json\"))\n",
    "\n",
    "with open(random_song, \"rb\") as f:\n",
    "    file_info = pickle.load(f)\n",
    "    query_embed = file_info.get(\"predictions_87\")\n",
    "    \n",
    "search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "result = collection.search([query_embed], \"predictions\", search_params, limit=10,offset=1, output_fields=[\"path\"])\n",
    "\n",
    "for element in result[0]:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## embedding_512 ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embeddings_512', 'predictions_87']\n"
     ]
    }
   ],
   "source": [
    "print(utility.list_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['predictions_87']\n"
     ]
    }
   ],
   "source": [
    "# Check if the collection exists\n",
    "check_collection_name = 'embeddings_512'\n",
    "\n",
    "check_collection = utility.has_collection(check_collection_name )\n",
    "if check_collection:\n",
    "    drop_result = utility.drop_collection(check_collection_name )\n",
    "\n",
    "print(utility.list_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating example collection: embeddings_512\n",
      "Schema: {'auto_id': False, 'description': '', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': False}, {'name': 'path', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 280}}, {'name': 'title', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 220}}, {'name': 'album', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 240}}, {'name': 'artist', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 120}}, {'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 512}}, {'name': 'top_5_genres', 'description': '', 'type': <DataType.ARRAY: 22>, 'params': {'max_length': 100, 'max_capacity': 5}, 'element_type': <DataType.VARCHAR: 21>}], 'enable_dynamic_field': True}\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# create a collection for embeddings_512\n",
    "collection_name = \"embeddings_512\"\n",
    "dimension = 512\n",
    "\n",
    "# Define the schema\n",
    "schema = CollectionSchema(\n",
    "    fields=[\n",
    "        FieldSchema(name=\"id\",dtype=DataType.INT64,is_primary=True,auto_id=False,max_length=100),\n",
    "        FieldSchema(name=\"path\", dtype=DataType.VARCHAR, max_length=280),\n",
    "        FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=220),\n",
    "        FieldSchema(name=\"album\", dtype=DataType.VARCHAR, max_length=240),\n",
    "        FieldSchema(name=\"artist\", dtype=DataType.VARCHAR, max_length=120),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dimension),\n",
    "        FieldSchema(name=\"top_5_genres\",dtype=DataType.ARRAY,element_type=DataType.VARCHAR,max_capacity=5,max_length=100),\n",
    "    ],\n",
    "    enable_dynamic_field=True,  # Optional, defaults to 'False'.\n",
    ")\n",
    "\n",
    "print(f\"Creating example collection: {collection_name}\")\n",
    "collection = Collection(collection_name, schema)\n",
    "print(f\"Schema: {schema}\")\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(args):\n",
    "    i, path = args\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            filepath = data.get(\"filepath\")\n",
    "            title = data.get(\"title\")\n",
    "            album = data.get(\"album\")\n",
    "            artist = data.get(\"artist\")\n",
    "            predictions = data.get(\"embedding_512\") #\n",
    "            top_5_genres = data.get(\"top_5_genres\")\n",
    "\n",
    "        if isinstance(predictions, np.ndarray):\n",
    "            return (i, filepath, title, album, artist, predictions, top_5_genres)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 18 records into collection embeddings_512.\n",
      "Inserted 36 records into collection embeddings_512.\n",
      "Inserted 54 records into collection embeddings_512.\n",
      "Inserted 72 records into collection embeddings_512.\n",
      "Inserted 90 records into collection embeddings_512.\n",
      "Inserted 108 records into collection embeddings_512.\n",
      "Inserted 126 records into collection embeddings_512.\n",
      "Inserted 144 records into collection embeddings_512.\n",
      "Inserted 162 records into collection embeddings_512.\n",
      "Inserted 180 records into collection embeddings_512.\n"
     ]
    }
   ],
   "source": [
    "# Use a multiprocessing pool to process the files in parallel\n",
    "with Pool() as p:\n",
    "    results = p.map(process_file, enumerate(pkl_files))\n",
    "\n",
    "# Filter out None results\n",
    "results = [r for r in results if r is not None]\n",
    "\n",
    "# Batch size\n",
    "batch_size = 18\n",
    "\n",
    "fails = []\n",
    "\n",
    "# Insert the data into the collection\n",
    "for i in range(0, len(results), batch_size):\n",
    "    id_batch, path_batch, title_batch, album_batch, artist_batch, embeddings_batch, top_5_genres_batch = zip(*results[i : i + batch_size])\n",
    "    documents = [\n",
    "        {\n",
    "            \"id\": id,\n",
    "            \"path\": path,\n",
    "            \"title\": title,\n",
    "            \"album\": album,\n",
    "            \"artist\": artist,\n",
    "            \"embedding\": embeddings.tolist(),\n",
    "            \"top_5_genres\": top_5_genres\n",
    "        }\n",
    "        for id, path, title, album, artist, embeddings, top_5_genres in zip(\n",
    "            id_batch, path_batch, title_batch, album_batch, artist_batch, embeddings_batch, top_5_genres_batch,\n",
    "        )\n",
    "    ]\n",
    "    try:\n",
    "        collection.insert(documents)\n",
    "        print(f\"Inserted {i + batch_size} records into collection {collection.name}.\")\n",
    "    except Exception as e:\n",
    "        fails.append((i + batch_size, documents))\n",
    "        print(f\"Error inserting batch {i + batch_size} into collection {collection.name}. Error: {str(e)}\")\n",
    "\n",
    "# 2m30s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed batches: []\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed batches: {fails}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(code=0, message=)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = {\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\"nlist\": 128},\n",
    "}\n",
    "collection.create_index(field_name=\"embedding\", index_params=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random mp3: Music_Dataset/Damian_Marley/11_-_Me_Name_Jr._Gong.pkl\n",
      "['reggae', 'dub', 'hiphop', 'world', 'electronic']\n",
      "\n",
      "\n",
      "id: 69, distance: 24.275529861450195, entity: {'path': 'Music_Dataset/Damian_Marley/06_-_Kingston_12.mp3'}\n",
      "id: 71, distance: 24.611507415771484, entity: {'path': 'Music_Dataset/Damian_Marley/03_-_10,000_Chariots.mp3'}\n",
      "id: 138, distance: 29.747173309326172, entity: {'path': 'Music_Dataset/Groundation/01_fight_all_you_can.mp3'}\n",
      "id: 37, distance: 29.939144134521484, entity: {'path': 'Music_Dataset/Tanya_Stephens/Tanya Stephens - These_Streets.mp3'}\n",
      "id: 93, distance: 30.485294342041016, entity: {'path': 'Music_Dataset/Dub_Incorporation/04 - Rudeboy.mp3'}\n",
      "id: 22, distance: 30.70628547668457, entity: {'path': 'Music_Dataset/Nneka/nneka - africans.mp3'}\n",
      "id: 113, distance: 30.7811279296875, entity: {'path': 'Music_Dataset/Winston_McAnuff/06_Sort_Me_Out.mp3'}\n",
      "id: 34, distance: 31.458003997802734, entity: {'path': 'Music_Dataset/Ben_Harper/01_-_Ben_harper_-_With_my_two_hands.mp3'}\n",
      "id: 95, distance: 32.321231842041016, entity: {'path': 'Music_Dataset/Dub_Incorporation/11 - Diversite.mp3'}\n",
      "id: 100, distance: 33.424564361572266, entity: {'path': 'Music_Dataset/Rockamovya/02 - Ya Better Rally.mp3'}\n"
     ]
    }
   ],
   "source": [
    "random_song = pick_random_mp3(DATASET).with_suffix(\".pkl\")\n",
    "print(f\"Random mp3: {random_song}\")\n",
    "print(get_top_5_genres(random_song, \"mtg_jamendo_genre.json\"))\n",
    "\n",
    "with open(random_song, \"rb\") as f:\n",
    "    file_info = pickle.load(f)\n",
    "    query_embed = file_info.get(\"embedding_512\")\n",
    "    \n",
    "search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "result = collection.search([query_embed], \"embedding\", search_params, limit=10, offset=1, output_fields=[\"path\"])\n",
    "print('\\n')\n",
    "for element in result[0]:\n",
    "    print(element)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
